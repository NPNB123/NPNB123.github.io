

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Transformer结构及简单pytorch实现 - </title>

  <meta name="description" content="介绍 Transformer 的结构及一个简单的PyTorch实现和训练代码">
  <meta name="author" content="NPNB"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "NPNB\u0027s Blog",
    
    "url": "https:\/\/NPNB123.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/NPNB123.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/NPNB123.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/NPNB123.github.io\/post\/transformer%E6%9E%B6%E6%9E%84%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0\/",
          "name": "Transformer结构及简单pytorch实现"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "NPNB"
  },
  "headline": "Transformer结构及简单pytorch实现",
  "description" : "介绍 Transformer 的结构及一个简单的PyTorch实现和训练代码\n",
  "inLanguage" : "en",
  "wordCount":  959 ,
  "datePublished" : "2024-12-08T19:28:21\u002b08:00",
  "dateModified" : "2024-12-08T19:28:21\u002b08:00",
  "image" : "https:\/\/NPNB123.github.io\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/NPNB123.github.io\/post\/transformer%E6%9E%B6%E6%9E%84%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/NPNB123.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/NPNB123.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="Transformer结构及简单pytorch实现" />
<meta property="og:description" content="介绍 Transformer 的结构及一个简单的PyTorch实现和训练代码">
<meta property="og:image" content="https://NPNB123.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://NPNB123.github.io/post/transformer%E6%9E%B6%E6%9E%84%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="NPNB&#39;s Blog" />

  <meta name="twitter:title" content="Transformer结构及简单pytorch实现" />
  <meta name="twitter:description" content="介绍 Transformer 的结构及一个简单的PyTorch实现和训练代码">
  <meta name="twitter:image" content="https://NPNB123.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://NPNB123.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.140.2">
  <link rel="alternate" href="https://NPNB123.github.io/index.xml" type="application/rss+xml" title="NPNB&#39;s Blog"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://NPNB123.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://NPNB123.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://NPNB123.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://NPNB123.github.io/">NPNB&#39;s Blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/page/about/">About</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="NPNB&#39;s Blog" href="https://NPNB123.github.io/">
            <img class="avatar-img" src="https://NPNB123.github.io/img/avatar-icon.png" alt="NPNB&#39;s Blog" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Transformer结构及简单pytorch实现</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on 12/08/2024
  
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;959&nbsp;words
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;NPNB
    
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>介绍 Transformer 的结构及一个简单的PyTorch实现和训练代码</p>
<p>Transformer 是在论文 <a href="https://arxiv.org/pdf/1706.03762">attention is all you need</a> 中所提出的一个seq2seq模型</p>
<h2 id="transformer结构图">Transformer结构图</h2>
<p><img src="../assets/image-20241208203130737.png" alt="transformer"></p>
<p><strong>Transformer由Encode和Decode两部分组成</strong></p>
<p><img src="../assets/image-20241208212505320.png" alt="encode_and_decode"></p>
<p>Encode 把输入处理成隐藏层, Decode 把Encode获得的信息处理成自然语言序列</p>
<h2 id="encode的结构">Encode的结构</h2>
<p><img src="../assets/%E8%AF%8D%E5%B5%8C%E5%85%A5.png" alt="词嵌入"></p>
<p>Encode中包含:</p>
<ol>
<li>词嵌入</li>
<li>位置编码</li>
<li>多头自注意力</li>
<li>残差连接及层归一化</li>
<li>前馈神经网络</li>
</ol>
<h3 id="词嵌入input-embedding">词嵌入(Input Embedding)</h3>
<p>每个输入词语通过一个词嵌入层（Embedding Layer）映射到一个固定维度的连续向量空间。</p>
<p>在实现词嵌入时往往可以利用PyTorch的<code>torch.nn.Embedding(vocab_size, d_model)</code></p>
<hr>
<h3 id="位置编码positional-encoding">位置编码(Positional Encoding)</h3>
<p>由于 Transformer 模型本身是基于自注意力机制的，没有内建的顺序信息，它不能直接获取输入序列的顺序。为了解决这个问题，Transformer 引入了位置编码（Positional Encoding）。</p>
<p>位置编码的目的是, 将位置信息附加到原始的信息上.</p>
<p>位置编码的计算方式:
<img src="../assets/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E8%AE%A1%E7%AE%97.png" alt="位置编码计算"></p>
<p>例子:<img src="../assets/image-20241126190745844.png" alt="image-20241126190745844"></p>
<p>pos表示词在序列中的位置:</p>
<p>Are -&gt; 0      you -&gt; 1      OK -&gt; 2       ? -&gt; 3</p>
<p>d表示位置编码向量的总维度数 此时是4</p>
<p>i表示位置编码向量维度索引的一半</p>
<p>可以得到位置编码:<img src="../assets/image-20241126191151315.png" alt="image-20241126191151315"></p>
<p><strong>词嵌入 + 位置编码 极大丰富了输入特征</strong></p>
<p>关于位置编码的一些问题:</p>
<ol>
<li>位置嵌入不会破坏词向量本身的信息吗?</li>
</ol>
<p>不会, 我们用远大于&quot;词向量 + 位置编码&quot;数量的训练数据和足够深的神经网络</p>
<p><img src="../assets/image-20241126185318483.png" alt="image-20241126185318483"></p>
<ol start="2">
<li>若出现了&quot;碰撞&quot; 即一个词向量+该位置的位置编码 恰好等于 另一个词向量与此位置的位置编码 会怎样?</li>
</ol>
<p>无所谓 每个词都是高维表示 哪怕一个维度出现碰撞 其他维度也不同 所有维度同时出现碰撞的概率几乎为0</p>
<p>代码实现Positional_Encoding</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Positional_Encoding的计算</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">drop_out</span><span class="p">,</span> <span class="n">max_len</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">drop_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d_model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span><span class="p">(</span><span class="n">j</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pe</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">j</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pe</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># tens: [seq_len, batch_size, d_model</span>
</span></span><span class="line"><span class="cl">        <span class="n">tens</span> <span class="o">=</span> <span class="n">tens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:</span><span class="n">tens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tens</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<h3 id="多头自注意力mulit-head-self-attention">多头自注意力(Mulit-head self attention)</h3>
<h4 id="自注意力">自注意力</h4>
<p>自注意力就是通过计算输入序列中各元素之间的关系（注意力），使得模型能够在处理某一个元素时，关注与之相关的其他元素，从而捕捉到更丰富的上下文信息。</p>
<p>我们定义三个矩阵 $$W_q, W_k, W_v$$使用这三个矩阵分别对所有的字向量进行三次线性变换，于是所有的字向量又衍生出三个新的向量q, k, v。我们将所有的q向量拼成一个大矩阵，记作<strong>查询矩阵 Q</strong>，将所有的k向量拼成一个大矩阵，记作<strong>键矩阵 K</strong>，将所有的 v向量拼成一个大矩阵，记作<strong>值矩阵 V</strong></p>
<p>$$即Q=W_q⋅x_i,  K=W_k⋅x_i,  V=W_v⋅x_i $$</p>
<p>为了获得第一个字的注意力权重，我们需要用第一个字的<strong>查询向量</strong> q1 乘以<strong>键矩阵 K</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">            [0, 4, 2]
</span></span><span class="line"><span class="cl">[1, 0, 2] x [1, 4, 3] = [2, 4, 4]
</span></span><span class="line"><span class="cl">            [1, 0, 1]
</span></span></code></pre></div><p>之后还需要将得到的值经过 softmax，使得它们的和为 1</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">softmax([2, 4, 4]) = [0.0, 0.5, 0.5]
</span></span></code></pre></div><p>有了权重之后，将权重其分别乘以对应字的<strong>值向量</strong> v</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="mf">0.0</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.5</span> <span class="o">*</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.5</span> <span class="o">*</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
</span></span></code></pre></div><p>最后将这些<strong>权重化后的值向量求和</strong>，得到第一个字的输出</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">  [0.0, 0.0, 0.0]
</span></span><span class="line"><span class="cl">+ [1.0, 4.0, 0.0]
</span></span><span class="line"><span class="cl">+ [1.0, 3.0, 1.5]
</span></span><span class="line"><span class="cl">-----------------
</span></span><span class="line"><span class="cl">= [2.0, 7.0, 1.5]
</span></span></code></pre></div><p>对其它的输入向量也执行相同的操作，即可得到通过 self-attention 后的所有输出</p>
<p>以上是对单个字的处理, 对于一整个输出, 我们用矩阵进行处理, 一次计算得出全部结果</p>
<p><img src="../assets/c7wF9x.png" alt="img">$$将 Q 和 K^T 相乘，然后除以 d_k，经过 softmax 以后再乘以 V 得到输出$$<img src="../assets/c7wk36.png" alt="img"></p>
<h4 id="多头自注意力multi-head-attention">多头自注意力(Multi-Head Attention)</h4>
<p>以上都是在介绍自注意力, 而多头自注意力就是定义多组Q, K, V, 让他们分别关注不同的上下文. 每组Q, K, V得到的结果相加即最终结果</p>
<hr>
<h3 id="残差连接与层归一化">残差连接与层归一化</h3>
<p>上一步中我们得到的最终结果, 即各个字对每个字的注意力矩阵后, 将其与开始时做完位置编码的矩阵X相加. 这就是残差链接</p>
<p>层归一化的作用是把神经网络中隐藏层归一为标准正态分布，以起到加快训练速度，加速收敛, 防止梯度爆炸与梯度消失的作用</p>
<p>计算出矩阵中每列的均值和方差
$$
\mu = \frac{1}{d} \sum_{i=1}^{d} x_i
\
\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2
$$</p>
<p>然后用<strong>每一列</strong>的<strong>每一个元素</strong>减去<strong>这列的均值</strong>，再除以<strong>这列的标准差</strong>，从而得到归一化后的数值，加 ϵ 是为了防止分母为 0
$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$
至此完成矩阵的层归一化</p>
<hr>
<h3 id="前馈神经网络">前馈神经网络</h3>
<p>Transformer中的前馈神经网络一般由两个线性变换（即全连接层）和一个激活函数组成. 用来增强模型表达能力以及提供非线性转换, 使模型能够学习更复杂的特征.</p>
<p><strong>至此Encode的结构搭建完成</strong></p>
<hr>
<h2 id="decode的结构">Decode的结构</h2>
<p><img src="../assets/Decode.png" alt="Masked 多头自注意力 (1)"></p>
<p>Decode 中和 Encode中有很多相同的部分, 比如 位置编码与词嵌入, 前馈神经网络, 残差连接等等, 这里不再重复, 重点放在Encode中没有的两个结构:</p>
<ol>
<li>Masked Multi-Head Self-Attention</li>
<li>Encoder-Decoder Attention</li>
</ol>
<h3 id="masked-multi-head-self-attention">Masked Multi-Head Self-Attention</h3>
<p>我们要求Decode需要遵循一定的时序规则. 即在每个词预测时应该仅依赖于之前的词，而不能看到未来的词为了满足这个需求, Masked Multi-Head Self-Attention会在计算注意力时, 对未来的词进行mask, 使得Decode在处理每个词时只能看到它之前的词, 而看不到后面的词, 确保生成模型的因果性.</p>
<p>在前面的操作上和普通的多头自注意力都一样, 输入的字符串在经过词嵌入和位置编码后, 进行三次线性变换得到矩阵Q, K, V, 然后进行self-Attention操作得到  Scaled Scores . 接下来进行mask, 非常简单, 就是生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可.<img src="../assets/U3FCQ0.png" alt="img"></p>
<p>之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重</p>
<hr>
<h3 id="masked-encoder-decoder-attention">Masked Encoder-Decoder Attention</h3>
<p>这部分的计算流程和Masked Multi-Head Self-Attention很像, 只是不需要对输入X进行三次线性变换, 而是Q就是Masked Multi-Head Self-Attention的输出, 而K, V时Encoder的输出.</p>
<h2 id="完成">完成</h2>
<p>在Transformer中, Encoder和Decoder都可以有多层叠在一起. 整个的内容可以被总结在这样的一张图中</p>
<p><img src="../assets/c7w7rD.png" alt="img"></p>
<h2 id="pytorch的简单实现">Pytorch的简单实现</h2>
<p>在Pytorch中, 已经有了transformer模块. 所以实现一个transformer很简单.</p>
<p>我们来实现一个简单的中译英模型及训练</p>
<p>库引入及模型config</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">json</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">datetime</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pickle</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">Data</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tiktoken._educational</span> <span class="kn">import</span> <span class="n">SimpleBytePairEncoding</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tiktoken._educational_cn</span> <span class="kn">import</span> <span class="n">SimpleBytePairEncodingCN</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># model_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl"><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>
</span></span><span class="line"><span class="cl"><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl"><span class="n">n_heads</span> <span class="o">=</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl"><span class="n">drop_out</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl"><span class="n">src_len</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl"><span class="n">tgt_len</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">max_len</span> <span class="o">=</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># train_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl"><span class="n">num_epoch</span> <span class="o">=</span> <span class="mi">12</span>
</span></span><span class="line"><span class="cl"><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span></span><span class="line"><span class="cl"><span class="n">epoch_size_to_update</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># scheduler的参数, 每epoch_size_to_update的epoch后更新lr, 更新为lr*gamma</span>
</span></span><span class="line"><span class="cl"><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl"><span class="n">betas_1</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># 控制动量（momentum）的计算</span>
</span></span><span class="line"><span class="cl"><span class="n">betas_2</span> <span class="o">=</span> <span class="mf">0.98</span>
</span></span><span class="line"><span class="cl"><span class="n">PAD_ID</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># [PAD] 的 token序号</span>
</span></span><span class="line"><span class="cl"><span class="n">steps</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c1"># 每steps个batch输出一次损失</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># tokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">4096</span>
</span></span><span class="line"><span class="cl"><span class="n">pattern</span> <span class="o">=</span> <span class="s2">&#34;&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d| ?[\p</span><span class="si">{L}</span><span class="s2">]+| ?[\p</span><span class="si">{N}</span><span class="s2">]+| ?[^\s\p</span><span class="si">{L}</span><span class="s2">\p</span><span class="si">{N}</span><span class="s2">\[\]]+|\s+(?!\S)|\s+|\[.*?\]&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># device</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;cuda:0&#34;</span>
</span></span></code></pre></div><p>tokenizer使用的是魔改版tiktoken中_educational里的SimpleBytePairEncoding</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;cn_vocab.pkl&#34;</span><span class="p">,</span> <span class="s2">&#34;rb&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">cn_tokenized_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">cn_tokenizer</span> <span class="o">=</span> <span class="n">SimpleBytePairEncodingCN</span><span class="p">(</span><span class="n">pat_str</span><span class="o">=</span><span class="n">pattern</span><span class="p">,</span> <span class="n">mergeable_ranks</span><span class="o">=</span><span class="n">cn_tokenized_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;en_vocab.pkl&#34;</span><span class="p">,</span> <span class="s2">&#34;rb&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">en_tokenized_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">en_tokenizer</span> <span class="o">=</span> <span class="n">SimpleBytePairEncoding</span><span class="p">(</span><span class="n">pat_str</span><span class="o">=</span><span class="n">pattern</span><span class="p">,</span> <span class="n">mergeable_ranks</span><span class="o">=</span><span class="n">en_tokenized_data</span><span class="p">)</span>
</span></span></code></pre></div><p>加载训练模型的数据以及处理</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 处理数据集的数据, 加上[PAD]等特殊符号</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载训练模型数据</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;test.jsonl&#34;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">datas</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">datas</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;load done </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">datasets</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pad_id</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [START]: starting of decoding input</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [EOS]: starting of decoding output</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [PAD]: fill in blank sequence if current batch data size is short than time steps</span>
</span></span><span class="line"><span class="cl"><span class="n">enc_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cn_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&#34;chinese&#34;</span><span class="p">])</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">dec_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">en_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&#34;[START]&#34;</span><span class="o">+</span><span class="n">item</span><span class="p">[</span><span class="s2">&#34;english&#34;</span><span class="p">])</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">dec_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">en_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&#34;english&#34;</span><span class="p">]</span><span class="o">+</span><span class="s2">&#34;[EOS]&#34;</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;begin fill pad&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#fill with [PAD]</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_enc_inputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_dec_inputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_dec_outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_enc_inputs</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">enc</span> <span class="o">+</span> <span class="n">pad_id</span> <span class="o">*</span> <span class="p">(</span><span class="n">src_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">enc</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">src_len</span> <span class="k">else</span> <span class="n">enc</span><span class="p">[:</span><span class="n">src_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">enc</span> <span class="ow">in</span> <span class="n">enc_inputs</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_dec_inputs</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">dec</span> <span class="o">+</span> <span class="n">pad_id</span> <span class="o">*</span> <span class="p">(</span><span class="n">tgt_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">dec</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dec</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tgt_len</span> <span class="k">else</span> <span class="n">dec</span><span class="p">[:</span><span class="n">tgt_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dec</span> <span class="ow">in</span> <span class="n">dec_inputs</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_dec_outputs</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">dec</span> <span class="o">+</span> <span class="n">pad_id</span> <span class="o">*</span> <span class="p">(</span><span class="n">tgt_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">dec</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dec</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tgt_len</span> <span class="k">else</span> <span class="n">dec</span><span class="p">[:</span><span class="n">tgt_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dec</span> <span class="ow">in</span> <span class="n">dec_outputs</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">pssd_enc_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pssd_enc_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_dec_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pssd_dec_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pssd_dec_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pssd_dec_outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 配置dataloader</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyDataSet</span><span class="p">(</span><span class="n">Data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">super</span><span class="p">(</span><span class="n">MyDataSet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">enc_inputs</span> <span class="o">=</span> <span class="n">enc_inputs</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">dec_inputs</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">dec_outputs</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_outputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">loader</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">MyDataSet</span><span class="p">(</span><span class="n">pssd_enc_inputs</span><span class="p">,</span> <span class="n">pssd_dec_inputs</span><span class="p">,</span> <span class="n">pssd_dec_outputs</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></div><p>然后是重点, 用torch.nn中的transformer来实现一个简单的transformer模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 模型结构</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Positional_Encoding计算</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">drop_out</span><span class="p">,</span> <span class="n">max_len</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">drop_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d_model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span><span class="p">(</span><span class="n">j</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pe</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">j</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pe</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># tens: [seq_len, batch_size, d_model</span>
</span></span><span class="line"><span class="cl">        <span class="n">tens</span> <span class="o">=</span> <span class="n">tens</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:</span><span class="n">tens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 构造模型</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MyTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 使用到了nn.Transformer</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">d_ff</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">nhead</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_encoder_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_decoder_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">enc_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dec_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">drop_out</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        enc_inputs: [batch_size, src_len]
</span></span></span><span class="line"><span class="cl"><span class="s1">        dec_inputs: [batch_size, tgt_len]
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">enc_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">src_len</span> <span class="o">=</span> <span class="n">enc_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">dec_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">enc_key_pad_mask</span> <span class="o">=</span> <span class="n">enc_inputs</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dec_key_pad_mask</span> <span class="o">=</span> <span class="n">dec_inputs</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">enc_dec_key_padding_mask</span> <span class="o">=</span> <span class="n">enc_key_pad_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">enc_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">enc_dec_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">dec_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">generate_square_subsequent_mask</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">enc_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">enc_embedding</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">dec_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dec_embedding</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">dec_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">src</span><span class="o">=</span><span class="n">enc_input</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">tgt</span><span class="o">=</span><span class="n">dec_input</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">src_mask</span><span class="o">=</span><span class="n">enc_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">tgt_mask</span><span class="o">=</span><span class="n">dec_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">memory_mask</span><span class="o">=</span><span class="n">enc_dec_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">enc_key_pad_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">dec_key_pad_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">enc_dec_key_padding_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dec_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">dec_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">dec_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dec_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span></code></pre></div><p>加载训练用的损失函数和梯度下降算法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">MyTransformer</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_ID</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">betas_1</span><span class="p">,</span> <span class="n">betas_2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">epoch_size_to_update</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
</span></span></code></pre></div><p>开始训练</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 训练</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_outputs</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">enc_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">dec_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dec_outputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Learning Rate after epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p>本文结束</p>

        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fNPNB123.github.io%2fpost%2ftransformer%25E6%259E%25B6%25E6%259E%2584%25E7%259A%2584pytorch%25E5%25AE%259E%25E7%258E%25B0%2f&amp;text=Transformer%e7%bb%93%e6%9e%84%e5%8f%8a%e7%ae%80%e5%8d%95pytorch%e5%ae%9e%e7%8e%b0&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fNPNB123.github.io%2fpost%2ftransformer%25E6%259E%25B6%25E6%259E%2584%25E7%259A%2584pytorch%25E5%25AE%259E%25E7%258E%25B0%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fNPNB123.github.io%2fpost%2ftransformer%25E6%259E%25B6%25E6%259E%2584%25E7%259A%2584pytorch%25E5%25AE%259E%25E7%258E%25B0%2f&amp;title=Transformer%e7%bb%93%e6%9e%84%e5%8f%8a%e7%ae%80%e5%8d%95pytorch%e5%ae%9e%e7%8e%b0" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fNPNB123.github.io%2fpost%2ftransformer%25E6%259E%25B6%25E6%259E%2584%25E7%259A%2584pytorch%25E5%25AE%259E%25E7%258E%25B0%2f&amp;title=Transformer%e7%bb%93%e6%9e%84%e5%8f%8a%e7%ae%80%e5%8d%95pytorch%e5%ae%9e%e7%8e%b0" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fNPNB123.github.io%2fpost%2ftransformer%25E6%259E%25B6%25E6%259E%2584%25E7%259A%2584pytorch%25E5%25AE%259E%25E7%258E%25B0%2f&amp;title=Transformer%e7%bb%93%e6%9e%84%e5%8f%8a%e7%ae%80%e5%8d%95pytorch%e5%ae%9e%e7%8e%b0" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fNPNB123.github.io%2fpost%2ftransformer%25E6%259E%25B6%25E6%259E%2584%25E7%259A%2584pytorch%25E5%25AE%259E%25E7%258E%25B0%2f&amp;description=Transformer%e7%bb%93%e6%9e%84%e5%8f%8a%e7%ae%80%e5%8d%95pytorch%e5%ae%9e%e7%8e%b0" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
          
            <li class="next">
              <a href="https://NPNB123.github.io/post/langchain%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95rag/" data-toggle="tooltip" data-placement="top" title="Langchain实现简单RAG">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      
      
      
      
      
        
      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
		
		  <a href="mailto:huozerun@qq.com" title="Email me">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://github.com/NPNB123" title="GitHub">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="https://npnb123.github.io/">NPNB</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://NPNB123.github.io/">NPNB&#39;s Blog</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.140.2</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://NPNB123.github.io/js/main.js"></script>
<script src="https://NPNB123.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://NPNB123.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

